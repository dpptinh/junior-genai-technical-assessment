
## Guide to Implementing and Optimizing RAG Systems
---

### 📚 Terminology
<details - open>
<summary>Definitions of key concepts and terms used in this document</summary>

---

- **RAG (Retrieval-Augmented Generation)**
  - An AI architecture that combines two stages: **Retrieval** of relevant information from a large knowledge base and **Generation** of an answer based on that information.
  - Goal: To minimize "hallucinations" and provide answers based on factual data.

- **ReAct (Reasoning and Acting)**
  - A framework that enables a Large Language Model (LLM) to perform complex tasks by iteratively following a cycle: **Thought** -> **Action** -> **Observation**.
  - The LLM can select and use external tools (e.g., web search, database queries) to gather necessary information before providing a final answer.

- **Multi-hop Reasoning**
  - The ability to reason across multiple steps or information sources to answer a complex question.
  - Example: To answer "Compare A and B," the system needs to find information about A, then find information about B, and finally synthesize the findings to make a comparison.

- **Hybrid Search**
  - A search technique that combines two or more methods to improve accuracy.
  - In this document, we combine:
    - **Dense Retrieval (Semantic Search)**: Searches based on the semantic meaning of the query, using vector embeddings.
    - **Sparse Retrieval (BM25)**: Searches based on keyword matching, effective for specific terms.

- **Vector Database**
  - A database optimized for storing and querying vector embeddings (numerical representations of text, images, etc.).
  - Examples: Chroma, Pinecone, Weaviate.

- **Retrieval Metrics (Recall@k, Precision@k, MAP@k, Hits@k)**
  - Metrics used to measure the effectiveness of the retrieval stage in RAG. They evaluate the accuracy and completeness of the documents retrieved by the system compared to a "ground-truth" dataset.

- **Generation Metrics (Answer Relevancy, Correctness, Similarity)**
  - Metrics used to evaluate the quality of the answer generated by the LLM, often measured using frameworks like RAGAS.

---
</details>

### 🛠 Implementation Guide: Building an End-to-End RAG Pipeline with a ReAct Agent
<details - open>
<summary>Steps to build a RAG system with multi-hop reasoning capabilities</summary>

---

#### 🧭 System Architecture and Logic Flow Overview

- **Objective**: Build an intelligent agent capable of answering complex questions by:
  - **1. Querying an internal knowledge base**: Searching for information in provided documents.
  - **2. Searching the web**: If information is not available in the knowledge base, the agent automatically searches the Internet.
  - **3. Multi-hop reasoning**: The agent breaks down complex questions, performs multiple searches, and synthesizes the results to formulate a final answer.
- **Workflow (ReAct Framework)**:
  - The diagram below illustrates the **Thought -> Action -> Observation** cycle of the ReAct Agent.
    ```mermaid
    flowchart TD
        Q([🔎 Query]) --> T{{🧠 Thinking}}
        T -- Action --> TOOL([🛠️ Tool: Knowledge Base / Web Search])
        TOOL --> OBS([👁️ Observation])
        OBS --> T
        T -- Finish --> A([📝 Answer])

        %% Colors & Style
        style Q fill:#fca5a5,stroke:#ef4444,stroke-width:2px,color:#000
        style T fill:#86efac,stroke:#22c55e,stroke-width:2px,color:#000
        style TOOL fill:#e5e7eb,stroke:#6b7280,stroke-width:2px,color:#000
        style OBS fill:#e0f2fe,stroke:#38bdf8,stroke-width:2px,color:#000
        style A fill:#fde68a,stroke:#f59e0b,stroke-width:2px,color:#000
    ```

---

#### 🧱 Tech Stack Analysis

- **Table of technology components**:
  - The table below describes the role of each tool and framework in the pipeline.
    | Layer | Tool / Framework | Purpose |
    | :--- | :--- | :--- |
    | **Document Ingestion** | `SimpleDirectoryReader` (LlamaIndex) | Loads raw files from a local directory. |
    | **Chunking & Embedding** | `SentenceSplitter`, `HuggingFaceEmbedding` | Splits documents into semantic chunks and converts them into vectors. |
    | **Vector Storage** | `ChromaVectorStore` + Chroma DB | Stores and queries vector embeddings. |
    | **Semantic Search** | `VectorIndexRetriever` | Retrieves relevant text chunks based on vector similarity. |
    | **Tool Abstraction** | LangChain `Tool` | Wraps the retriever and search tool into callable functions. |
    | **External Knowledge Tool** | `DuckDuckGoSearchRun` | Performs web searches when information is not available in the knowledge base. |
    | **Reasoning Engine** | `ReAct Agent` (LangChain) | Allows the agent to select and execute tools based on its reasoning steps. |
    | **Language Model** | `Gemini` | Used for both reasoning and generating the final answer. |

---

#### 🧩 Step-by-Step Implementation Guide with Code



**Step 1: Load and Embed Documents (Ingestion & Embedding)**
- **Purpose**: Read data from a directory, split it into chunks, and create vector embeddings for each chunk to store in ChromaDB.
- **Code**:
  ```python
  from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext
  from llama_index.core.node_parser import SentenceSplitter
  from llama_index.core.embeddings import HuggingFaceEmbedding
  from llama_index.vector_stores.chroma import ChromaVectorStore
  import chromadb


  def load_and_index_docs(folder_path):
      documents = SimpleDirectoryReader(folder_path).load_data()
      node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)
      embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-large-en-v1.5")
      service_context = ServiceContext.from_defaults(embed_model=embed_model, node_parser=node_parser)
      chroma_client = chromadb.Client()
      vector_store = ChromaVectorStore(chroma_collection=chroma_client.create_collection("rag_docs"))
      index = VectorStoreIndex.from_documents(documents, service_context=service_context, vector_store=vector_store)
      return index
  ```

**Step 2: Set Up the Query Engine and Retriever**
- **Purpose**: Create a `Retriever` to perform vector search and a `QueryEngine` to encapsulate the query logic.
- **Code**:
  ```python
  from llama_index.core.query_engine import RetrieverQueryEngine
  from llama_index.core.retrievers import VectorIndexRetriever

  def setup_query_engine(index):
      retriever = VectorIndexRetriever(index=index, similarity_top_k=5)
      query_engine = RetrieverQueryEngine(retriever=retriever)
      return query_engine, retriever
  ```

**Step 3: Wrap the Retriever as a LangChain Tool**
- **Purpose**: Transform the `QueryEngine` into a "Tool" that the ReAct Agent can understand and use.
- **Code**:
  ```python
  from langchain.agents import Tool

  def build_qa_tool(query_engine):
      def query_fn(q):
          return query_engine.query(q).response

      return Tool(
          name="knowledge_base_lookup",
          func=query_fn,
          description="Use this tool to query internal knowledge base."
      )
  ```

**Step 4: Add a Web Search Tool (DuckDuckGo)**
- **Purpose**: Provide the agent with a fallback tool to search for general information on the Internet.
- **Code**:
  ```python
  from langchain.tools import DuckDuckGoSearchRun

  def build_search_tool():
      search = DuckDuckGoSearchRun()
      return Tool(
          name="web_search",
          func=search.run,
          description="Use this tool to search external general knowledge."
      )
  ```

**Step 5: Set Up the ReAct Agent**
- **Purpose**: Initialize the agent, provide it with a list of tools, and enable it to display its reasoning steps (`verbose=True`).
- **Code**:
  ```python
  from langchain.agents import initialize_agent, AgentType
  from langchain.llms import OpenAI

  def build_react_agent(tools):
      return initialize_agent(
          tools=tools,
          agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
          llm=OpenAI(temperature=0),
          verbose=True  # Show reasoning steps
      )
  ```

**Steps 6 & 7: Execute the Pipeline and Ask a Complex Question**
- **Purpose**: Combine all components and execute a question that requires multi-hop reasoning.
- **Code**:
  ```python
  def multi_hop_reasoning(agent, question):
      print("\n🤔 Reasoning Trace:")
      return agent.run(question)

  if __name__ == "__main__":
      index = load_and_index_docs("./data")
      query_engine, retriever = setup_query_engine(index)
      qa_tool = build_qa_tool(query_engine)
      search_tool = build_search_tool()
      agent = build_react_agent([qa_tool, search_tool])

      question = (
          "What are the key differences between Pinecone and Weaviate in terms of scalability and "
          "search algorithm, and how should I choose between them for a multi-tenant architecture?"
      )

      final_answer = multi_hop_reasoning(agent, question)
      print("\n🧠 Final Answer:\n", final_answer)
  ```

---

#### ✅ Multi-hop Reasoning Analysis

- **How it works**:
  - The ReAct agent receives a complex question and automatically breaks it down into smaller, logical steps.
  - At each step, the agent **thinks (Thought)** to decide which tool to use (`knowledge_base_lookup` or `web_search`).
  - After performing an **action (Action)**, it receives an **observation (Observation)** and uses this information for the next thought step.
- **Example of a reasoning flow**:
  - Below is an illustrative example of how the agent performs multiple "hops" to gather information.
    ```text
    > Thought: The user is asking for a comparison between Pinecone and Weaviate for a specific use case (multi-tenant). I should first check my internal knowledge base for information on each.
    > Action: knowledge_base_lookup
    > Action Input: "Pinecone scalability and search algorithm"
    ...
    > Observation: [Information about Pinecone's architecture]
    > Thought: Now I have information about Pinecone. I need to do the same for Weaviate to compare them.
    > Action: knowledge_base_lookup
    > Action Input: "Weaviate scalability and search algorithm for multi-tenant architecture"
    ...
    > Observation: [Information about Weaviate's features including multi-tenancy]
    > Thought: I now have details on both. I can synthesize this information to answer the user's full question.
    > Final Answer: [A comprehensive comparison is generated]
    ```

---
</details>

### 🔍 Evaluating and Optimizing RAG Performance
<details - open>
<summary>Methods for measuring, experimenting with, and improving the performance of a RAG system</summary>

---

#### 🎯 Evaluation Context and Strategy

- **Challenge**: Standard RAG evaluation frameworks like **RAGAS** often assume that each question has only one correct context chunk. This is not suitable for datasets like [MultiHop-RAG](https://openreview.net/pdf?id=t4eB3zYWBK), where a question requires multiple context chunks.
- **Strategy**:
  - **Stage 1 (Retrieval)**: Evaluate the system's retrieval capabilities using classic metrics like `Recall@k`, `Precision@k`, `MAP@k`, and `Hits@k`.
  - **Stage 2 (Generation)**: Evaluate the quality of the final answer using RAGAS metrics like `Answer Relevancy`, `Answer Correctness`, and `Answer Similarity`.

---

#### 📌 Retrieval Metrics

**Recall@k**
- **Definition**: Measures the percentage of actual relevant (ground-truth) documents that the system found within the top-k results.
- **Formula**:
  ```math
  Recall@k = \frac{|\text{Retrieved@k} \cap \text{GroundTruth}|}{|\text{GroundTruth}|}
  ```

**Precision@k**
- **Definition**: Measures the percentage of relevant documents among the top-k results returned by the system.
- **Formula**:
  ```math
  Precision@k = \frac{|\text{Retrieved@k} \cap \text{GroundTruth}|}{k}
  ```

**MAP@k (Mean Average Precision)**
- **Definition**: A more comprehensive metric, it is the mean of the Precision scores calculated at each position where a relevant document is found in the top-k. It rewards ranking relevant documents higher.
- **Formula**:
  ```math
  AP@k = \frac{1}{|\text{GT}|} \sum_{j=1}^{k} P(j) \cdot rel(j)
  ```
  ```math
  MAP@k = \frac{1}{N} \sum_{i=1}^{N} AP@k_i
  ```

**Hits@k**
- **Definition**: Measures the percentage of queries for which at least one relevant document is found within the top-k results.
- **Formula**:
  ```math
  Hits@k_i =
  \begin{cases}
  1 & \text{if } \exists r \in \text{Retrieved@k}_i \cap \text{GT}_i \\
  0 & \text{otherwise}
  \end{cases}
  ```
  ```math
  Hits@k = \frac{1}{N} \sum_{i=1}^{N} Hits@k_i
  ```

---

#### 📌 Generation Metrics (RAGAS)

- **Answer Semantic Similarity**:
  - **Purpose**: Evaluates the semantic similarity between the system-generated answer and the ground truth answer.
  - **Scale**: 0 to 1 (higher is better).
- **Answer Correctness**:
  - **Purpose**: Evaluates the factual accuracy of the answer compared to the ground truth answer.
  - **Scale**: 0 to 1 (higher is better).
- **Answer Relevancy**:
  - **Purpose**: Evaluates the relevance of the answer to the original question, penalizing incomplete answers or those containing redundant information.
  - **Scale**: 0 to 1 (higher is better).

---

#### 🧪 Experimentation and Optimization

**Setting Up the Experiment Environment**
- **Purpose**: Prepare the necessary components for experimentation, including the embedding model, vector store, and retriever.
- **Code**:
  ```python
  from langchain_community.vectorstores import Chroma
  from langchain.embeddings import HuggingFaceEmbeddings
  from langchain.retrievers import BM25Retriever
  from langchain.schema import Document
  from datasets import Dataset
  import concurrent.futures
  from typing import List, Dict, Tuple
  from langchain_google_genai import ChatGoogleGenerativeAI

  # Embedding model
  embedding = HuggingFaceEmbeddings(
      model_name="BAAI/bge-large-en-v1.5",
      model_kwargs={"device": "cuda"}
  )

  # Build vector store
  vectorstore = Chroma.from_texts(
      texts=texts,
      embedding=embedding,
      persist_directory="./chroma_store_db"
  )

  # LLM for generation
  llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.5)

  # Build BM25 retriever for hybrid search
  bm25_retriever = BM25Retriever.from_documents(documents, k=4)
  ```

**Experiment 1: Baseline - Semantic Search Only**
- **Method**: Use a pure semantic search approach with the `BAAI/bge-large-en-v1.5` model and ChromaDB.
- **Code**:
  ```python
  def semantic_retrieve(query: str, k: int = 4) -> List[str]:
      docs = vectorstore.similarity_search(query, k=k)
      return [doc.page_content for doc in docs]
  ```
- **Results**:
  ```text
  Recall@4:     0.5197
  Precision@4:  0.3320
  MAP@4:        0.4135
  Hits@4:       0.8440
  ```

**Experiment 2: Optimization - Hybrid Search (BM25 + Dense)**
- **Logic**: Combine scores from keyword search (BM25) and semantic search (Dense) to leverage the strengths of both.
  - **BM25**: Excels at finding exact terms and proper nouns.
  - **Dense**: Excels at understanding the intent and context of the query.
- **Score Combination Formula**:
  - Uses linear interpolation with a weight `alpha`.
    $$
    \text{HybridScore} = \alpha \cdot s_{\text{dense}} + (1 - \alpha) \cdot s_{\text{bm25}}
    $$
- **Code**:
  ```python
  def hybrid_retrieve(query: str, k: int = 4, alpha: float = 0.2) -> List[str]:
      bm25_docs = bm25_retriever.get_relevant_documents(query)
      dense_docs = vectorstore.similarity_search_with_score(query, k=k)

      # Normalize scores and combine
      bm25_dict = normalize_scores_with_rank(bm25_docs, "bm25")
      dense_dict = normalize_scores_with_value(dense_docs, "dense")

      all_keys = set(bm25_dict.keys()) | set(dense_dict.keys())
      scored = []
      for key in all_keys:
          s_bm25 = bm25_dict.get(key, {"score": 0.0})["score"]
          s_dense = dense_dict.get(key, {"score": 0.0})["score"]
          score = alpha * s_dense + (1 - alpha) * s_bm25
          doc = dense_dict.get(key, bm25_dict.get(key))["doc"]
          scored.append((doc, score))

      top_docs = sorted(scored, key=lambda x: x[1], reverse=True)[:k]
      return [doc.page_content for doc, _ in top_docs]
  ```
- **Results**:
  ```text
  Recall@4:     0.5283
  Precision@4:  0.3330
  MAP@4:        0.4262
  Hits@4:       0.8580
  ```

---

#### 📊 Performance Comparison and Analysis

- **Retrieval Results Comparison Table**:
  - The table below summarizes the improvements from applying Hybrid Search.
    | Metric | Semantic Only | Hybrid (α=0.2) | Improvement |
    | :--- | :--- | :--- | :--- |
    | **Recall@4** | 0.5197 | **0.5283** | +1.65% |
    | **Precision@4** | 0.3320 | **0.3330** | +0.30% |
    | **MAP@4** | 0.4135 | **0.4262** | +3.07% |
    | **Hits@4** | 0.8440 | **0.8580** | +1.66% |
- **Observations and Conclusion**:
  - **Clear Effectiveness**: The Hybrid Search technique improved all metrics, especially `MAP@4` and `Recall@4`. This demonstrates that combining keyword and semantic search helps the system find more correct documents and rank them higher.
  - **Optimal Choice**: Hybrid Search is a simple yet highly effective optimization technique and should be considered a primary improvement step for any practical RAG system.

---

#### ⚙️ Implementing the Generation Stage

- **Objective**: Compare two answer generation methods: Naive RAG and the ReAct Agent.

**Method 1: Naive RAG**
- **Logic**: Retrieve context using Hybrid Search, then feed the entire context and the question into a single prompt template for the LLM to generate an answer.
- **Code**:
  ```python
  def generate_naive_rag_responses(sampled_eval_data, hybrid_retrieve_func, ...):
      # ... (Implementation from original content)
      docs = hybrid_retrieve_func(question, k=k, alpha=alpha)
      context_text = "..." # Format documents into a string
      prompt = prompt_template.format(context=context_text, question=question)
      response = llm.invoke(prompt)
      # ...
  ```

**Method 2: ReAct Agent**
- **Logic**: Instead of just "stuffing" the context, the agent is given the `hybrid_retrieve` tool and decides when and how to use it based on its own reasoning.
- **Code**:
  ```python
  from langchain_core.tools import StructuredTool
  from langchain.agents import AgentExecutor, create_react_agent

  tools = [StructuredTool.from_function(hybrid_retrieve)]
  agent = create_react_agent(llm=llm, tools=tools, prompt=hub.pull("hwchase17/react"))
  agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

  # Invoke for each question
  response = agent_executor.invoke({"input": question})
  ```

**Evaluating Answers with RAGAS**
- **Purpose**: Use the RAGAS framework to evaluate the quality of the answers generated by the two methods above.
- **Code**:
  ```python
  from ragas import evaluate
  from ragas.metrics import answer_relevancy, answer_similarity, answer_correctness

  # Prepare dataset in Hugging Face's Dataset format
  data = {"question": questions, "answer": generated_answers, "ground_truth": ground_truths}
  dataset = Dataset.from_dict(data)

  # Run evaluation
  result = evaluate(
      dataset,
      metrics=[answer_relevancy, answer_similarity, answer_correctness],
      # ... other parameters
  )
  df = result.to_pandas()
  print(df.head())
  ```

---
</details>

---