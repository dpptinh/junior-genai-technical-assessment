### üõ† Implementation Guide: End-to-End RAG Pipeline with Multi-hop ReAct Agent

---

### üî∞ Overview
This guide demonstrates an end-to-end RAG pipeline:
- **LlamaIndex** for ingestion & retrieval
- **Chroma** as vector DB
- **LangChain ReAct Agent** for reasoning
- **DuckDuckGo Tool** as external search fallback
---

### üß≠ System Architecture Diagram
```mermaid
flowchart TD
    Q([üîé Query]) --> T{{üß† Thinking}}
    T -- Action --> TOOL([üõ†Ô∏è Tool])
    TOOL --> OBS([üëÅÔ∏è Observation])
    OBS --> T
    T -- Finish --> A([üìù Answer])

    %% Colors & Style
    style Q fill:#fca5a5,stroke:#ef4444,stroke-width:2px,color:#000
    style T fill:#86efac,stroke:#22c55e,stroke-width:2px,color:#000
    style TOOL fill:#e5e7eb,stroke:#6b7280,stroke-width:2px,color:#000
    style OBS fill:#e0f2fe,stroke:#38bdf8,stroke-width:2px,color:#000
    style A fill:#fde68a,stroke:#f59e0b,stroke-width:2px,color:#000


```

---

### üß± Tech Stack Overview

| Layer                    | Tool / Framework                     | Purpose                                                    |
|--------------------------|--------------------------------------|------------------------------------------------------------|
| Document Ingestion       | `SimpleDirectoryReader` (LlamaIndex) | Load raw files from local directory                        |
| Chunking & Embedding     | `SentenceSplitter`, `HuggingFaceEmbedding` | Convert docs to semantic chunks and vectors       |
| Vector Storage           | `ChromaVectorStore` + Chroma DB      | Store and query dense embeddings                           |
| Semantic Search          | `VectorIndexRetriever`               | Retrieve relevant chunks based on vector similarity        |
| Tool Abstraction         | LangChain `Tool`                     | Wrap retriever and search as callable tools                |
| External Knowledge Tool  | `DuckDuckGoSearchRun`                | Perform fallback searches for general queries              |
| Reasoning Engine         | `ReAct Agent` (LangChain)            | Enable step-by-step tool selection and execution           |
| Language Model           | `OpenAI`                             | Used for generation and reasoning                          |

---

### üß© Step-by-step Breakdown with Code

#### üü° Step 1: Load and Embed Documents
```python
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.embeddings import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

def load_and_index_docs(folder_path):
    documents = SimpleDirectoryReader(folder_path).load_data()
    node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)
    embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
    service_context = ServiceContext.from_defaults(embed_model=embed_model, node_parser=node_parser)
    chroma_client = chromadb.Client()
    vector_store = ChromaVectorStore(chroma_collection=chroma_client.create_collection("rag_docs"))
    index = VectorStoreIndex.from_documents(documents, service_context=service_context, vector_store=vector_store)
    return index
```

#### üü° Step 2: Create Query Engine & Retriever
```python
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever

def setup_query_engine(index):
    retriever = VectorIndexRetriever(index=index, similarity_top_k=5)
    query_engine = RetrieverQueryEngine(retriever=retriever)
    return query_engine, retriever
```

#### üü° Step 3: Wrap Retriever into LangChain Tool
```python
from langchain.agents import Tool

def build_qa_tool(query_engine):
    def query_fn(q):
        return query_engine.query(q).response

    return Tool(
        name="knowledge_base_lookup",
        func=query_fn,
        description="Use this tool to query internal knowledge base."
    )
```

#### üü° Step 4: Add Web Search Tool (DuckDuckGo)
```python
from langchain.tools import DuckDuckGoSearchRun

def build_search_tool():
    search = DuckDuckGoSearchRun()
    return Tool(
        name="web_search",
        func=search.run,
        description="Use this tool to search external general knowledge."
    )
```

#### üü° Step 5: Setup ReAct Agent (Multi-hop Execution Enabled)
```python
from langchain.agents import initialize_agent, AgentType
from langchain.llms import OpenAI

def build_react_agent(tools):
    return initialize_agent(
        tools=tools,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        llm=OpenAI(temperature=0),
        verbose=True  # Show reasoning steps
    )
```

#### üü° Step 6: Ask a Complex, Multi-hop Question
```python
def multi_hop_reasoning(agent, question):
    print("\nü§î Reasoning Trace:")
    return agent.run(question)
```

#### üß™ Step 7: Main Pipeline Runner
```python
if __name__ == "__main__":
    index = load_and_index_docs("./data")
    query_engine, retriever = setup_query_engine(index)
    qa_tool = build_qa_tool(query_engine)
    search_tool = build_search_tool()
    agent = build_react_agent([qa_tool, search_tool])

    question = (
        "What are the key differences between Pinecone and Weaviate in terms of scalability and "
        "search algorithm, and how should I choose between them for a multi-tenant architecture?"
    )

    final_answer = multi_hop_reasoning(agent, question)
    print("\nüß† Final Answer:\n", final_answer)
```

---

### ‚úÖ Notes on Multi-hop Reasoning
- The ReAct agent interprets the complex query and performs **step-by-step tool calls**.
- Intermediate reasoning ("Thought:", "Action:") is visible thanks to `verbose=True`.
- Example output clearly shows **multi-hop trace**:
```
> Thought: I need to know the vector DBs used for multi-tenant search
> Action: knowledge_base_lookup
...
> Thought: Let's check Weaviate‚Äôs open-source scalability limits
> Action: web_search
...
```
---
</details>

---

## üîç Evaluation and Optimization

ƒê·ªÉ ƒë√°nh gi√° hi·ªáu qu·∫£ truy xu·∫•t trong h·ªá th·ªëng RAG, ch√∫ng t√¥i s·ª≠ d·ª•ng t·∫≠p d·ªØ li·ªáu multi-hop t·ª´ [MultiHop-RAG](https://openreview.net/pdf?id=t4eB3zYWBK), trong ƒë√≥ m·ªói truy v·∫•n y√™u c·∫ßu nhi·ªÅu ƒëo·∫°n context li√™n quan. V√¨ v·∫≠y, framework nh∆∞ **RAGAS** kh√¥ng ph√π h·ª£p do gi·∫£ ƒë·ªãnh ch·ªâ m·ªôt ƒëo·∫°n context ƒë√∫ng.

### üéØ Evaluation Metrics

Ch√∫ng t√¥i s·ª≠ d·ª•ng 4 ch·ªâ s·ªë ph·ªï bi·∫øn trong ƒë√°nh gi√° h·ªá th·ªëng truy xu·∫•t th√¥ng tin:

---

### üìå **Recall\@k**

> ƒêo t·ª∑ l·ªá t√†i li·ªáu li√™n quan trong t·∫≠p top-k ƒë∆∞·ª£c truy xu·∫•t so v·ªõi t·ªïng s·ªë t√†i li·ªáu ground-truth.

**C√¥ng th·ª©c:**

```math
Recall@k = \frac{|\text{Retrieved@k} \cap \text{GroundTruth}|}{|\text{GroundTruth}|}
```

---

### üìå **Precision\@k**

> ƒêo t·ª∑ l·ªá t√†i li·ªáu li√™n quan trong top-k k·∫øt qu·∫£ so v·ªõi t·ªïng s·ªë t√†i li·ªáu ƒë∆∞·ª£c truy xu·∫•t.

```math
Precision@k = \frac{|\text{Retrieved@k} \cap \text{GroundTruth}|}{k}
```

---

### üìå **MAP\@k** (Mean Average Precision)

> Trung b√¨nh c√°c Precision t√≠nh t·∫°i t·ª´ng v·ªã tr√≠ c√≥ t√†i li·ªáu li√™n quan.

```math
AP@k = \frac{1}{|\text{GT}|} \sum_{j=1}^{k} P(j) \cdot rel(j)
```

```math
MAP@k = \frac{1}{N} \sum_{i=1}^{N} AP@k_i
```

---

### üìå **Hits\@k**

> ƒêo t·ª∑ l·ªá truy v·∫•n m√† √≠t nh·∫•t m·ªôt t√†i li·ªáu li√™n quan n·∫±m trong top-k.

```math
Hits@k_i =
\begin{cases}
1 & \text{if } \exists r \in \text{Retrieved@k}_i \cap \text{GT}_i \\
0 & \text{otherwise}
\end{cases}
```

```math
Hits@k = \frac{1}{N} \sum_{i=1}^{N} Hits@k_i
```

---

## üîß Setup

```python
from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import BM25Retriever
from langchain.schema import Document

from datasets import Dataset
import concurrent.futures
from typing import List, Dict, Tuple

# Embedding model
embedding = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",
    model_kwargs={"device": "cuda"}
)

# Build vector store
vectorstore = Chroma.from_texts(
    texts=texts,
    embedding=embedding,
    persist_directory="./chroma_store_db"
)

# Build BM25 retriever for hybrid
documents = [Document(page_content=text) for text in texts]
bm25_retriever = BM25Retriever.from_documents(documents, k=4)
```

---

## ‚úÖ Baseline: Semantic Search Only
S·ª≠ d·ª•ng m√¥ h√¨nh embedding BAAI/bge-large-en-v1.5 v·ªõi vector DB Chroma, v√† th·ª±c hi·ªán truy v·∫•n top-k=4 b·∫±ng semantic search.
```python
def semantic_retrieve(query: str, k: int = 4) -> List[str]:
    docs = vectorstore.similarity_search(query, k=k)
    return [doc.page_content for doc in docs]

def process_question_semantic(item: Dict) -> tuple:
    question = item["question"]
    contexts = semantic_retrieve(question, k=4)
    return question, contexts, item["contexts"]

def process_dataset_semantic(data: List[Dict]) -> Tuple:
    questions, contexts, gts = [], [], []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(process_question_semantic, item) for item in data]
        for f in concurrent.futures.as_completed(futures):
            q, ctxs, gt = f.result()
            questions.append(q)
            contexts.append(ctxs)
            gts.append(gt)
    return questions, contexts, gts
```

**K·∫øt qu·∫£:**

```text
Recall@4:     0.5197
Precision@4:  0.3320
MAP@4:        0.4135
Hits@4:       0.8440
```

---

## üîÄ Optimization: Hybrid Search (BM25 + Dense)
ƒê·ªÉ c·∫£i thi·ªán k·∫øt qu·∫£, ch√∫ng t√¥i √°p d·ª•ng k·ªπ thu·∫≠t **Hybrid Search** b·∫±ng c√°ch k·∫øt h·ª£p hai ph∆∞∆°ng ph√°p:

* **BM25 (sparse retrieval)** ‚Äî s·ª≠ d·ª•ng keyword matching
* **Dense (semantic retrieval)** ‚Äî d√πng embedding semantic similarity

S·ª≠ d·ª•ng c√¥ng th·ª©c **score interpolation** v·ªõi tham s·ªë `alpha = 0.2`:

$$
\text{HybridScore} = \alpha \cdot s_{\text{dense}} + (1 - \alpha) \cdot s_{\text{bm25}}
$$

Trong ƒë√≥:

- $s_{\text{dense}}$: normalized score t·ª´ semantic search
* $s_{\text{bm25}}$: normalized score t·ª´ BM25 

```python
def normalize_scores_with_rank(docs: List[Document], source: str) -> Dict[str, Dict]:
    n = len(docs)
    return {
        doc.page_content: {"doc": doc, "score": 1.0 - (i / (n - 1)), "source": source}
        for i, doc in enumerate(docs)
    } if n > 1 else {
        doc.page_content: {"doc": doc, "score": 1.0, "source": source}
        for doc in docs
    }

def normalize_scores_with_value(docs_with_scores: List[Tuple[Document, float]], source: str) -> Dict[str, Dict]:
    if not docs_with_scores:
        return {}
    scores = [s for _, s in docs_with_scores]
    min_s, max_s = min(scores), max(scores)
    range_s = max_s - min_s if max_s != min_s else 1.0
    return {
        doc.page_content: {"doc": doc, "score": (score - min_s) / range_s, "source": source}
        for doc, score in docs_with_scores
    }

def hybrid_retrieve(query: str, k: int = 4, alpha: float = 0.2) -> List[str]:
    bm25_docs = bm25_retriever.get_relevant_documents(query)
    dense_docs = vectorstore.similarity_search_with_score(query, k=k)

    bm25_dict = normalize_scores_with_rank(bm25_docs, "bm25")
    dense_dict = normalize_scores_with_value(dense_docs, "dense")

    all_keys = set(bm25_dict.keys()) | set(dense_dict.keys())
    scored = []
    for key in all_keys:
        s_bm25 = bm25_dict.get(key, {"score": 0.0})["score"]
        s_dense = dense_dict.get(key, {"score": 0.0})["score"]
        score = alpha * s_dense + (1 - alpha) * s_bm25
        doc = dense_dict.get(key, bm25_dict.get(key))["doc"]
        scored.append((doc, score))

    top_docs = sorted(scored, key=lambda x: x[1], reverse=True)[:k]
    return [doc.page_content for doc, _ in top_docs]
```

### üîÅ Evaluation

```python
def process_question_hybrid(item: Dict) -> tuple:
    question = item["question"]
    contexts = hybrid_retrieve(question, k=4, alpha=0.2)
    return question, contexts, item["contexts"]

def process_dataset_hybrid(data: List[Dict]) -> Tuple:
    questions, contexts, gts = [], [], []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(process_question_hybrid, item) for item in data]
        for f in concurrent.futures.as_completed(futures):
            q, ctxs, gt = f.result()
            questions.append(q)
            contexts.append(ctxs)
            gts.append(gt)
    return questions, contexts, gts
```

**K·∫øt qu·∫£:**

```text
Recall@4:     0.5283
Precision@4:  0.3330
MAP@4:        0.4262
Hits@4:       0.8580
```

---

## üìä So s√°nh K·∫øt qu·∫£

| Metric       | Semantic Only | Hybrid (Œ±=0.2) |
| ------------ | ------------- | -------------- |
| Recall\@4    | 0.5197        | **0.5283**     |
| Precision\@4 | 0.3320        | **0.3330**     |
| MAP\@4       | 0.4135        | **0.4262**     |
| Hits\@4      | 0.8440        | **0.8580**     |

---

## üß† Nh·∫≠n x√©t

Vi·ªác √°p d·ª•ng k·ªπ thu·∫≠t **hybrid retrieval** v·ªõi tr·ªçng s·ªë Œ± = 0.2 ƒë√£ gi√∫p c·∫£i thi·ªán hi·ªáu su·∫•t truy xu·∫•t ·ªü t·∫•t c·∫£ c√°c ch·ªâ s·ªë:

* **Recall** v√† **MAP** c·∫£i thi·ªán r√µ r·ªát, cho th·∫•y hybrid retrieval gi√∫p h·ªá th·ªëng bao ph·ªß nhi·ªÅu context ƒë√∫ng h∆°n.
* **Hits\@k** tƒÉng nh·∫π, ch·ª©ng minh r·∫±ng kh·∫£ nƒÉng l·∫•y √≠t nh·∫•t m·ªôt context ƒë√∫ng ƒë√£ ƒë∆∞·ª£c c·∫£i thi·ªán.
* **Precision** tƒÉng nh·∫π, kh√¥ng ƒë√°ng k·ªÉ do k c·ªë ƒë·ªãnh l√† 4.

> Hybrid search l√† m·ªôt k·ªπ thu·∫≠t ƒë∆°n gi·∫£n nh∆∞ng hi·ªáu qu·∫£ ƒë·ªÉ t·∫≠n d·ª•ng ∆∞u ƒëi·ªÉm c·ªßa c·∫£ BM25 (sparse lexical) v√† semantic retrieval (dense vector). ƒê√¢y l√† b∆∞·ªõc t·ªëi ∆∞u ƒë·∫ßu ti√™n n√™n th·ª±c hi·ªán trong b·∫•t k·ª≥ h·ªá th·ªëng RAG th·ª±c t·∫ø n√†o.

---
</details>
