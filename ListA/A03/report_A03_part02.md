### ğŸ›  Implementation Guide: End-to-End RAG Pipeline with Multi-hop ReAct Agent

---

### ğŸ”° Overview
This guide demonstrates an end-to-end RAG pipeline:
- **LlamaIndex** for ingestion & retrieval
- **Chroma** as vector DB
- **LangChain ReAct Agent** for reasoning
- **DuckDuckGo Tool** as external search fallback
---

### ğŸ§­ System Architecture Diagram
```mermaid
flowchart TD
    Q([ğŸ” Query]) --> T{{ğŸ§  Thinking}}
    T -- Action --> TOOL([ğŸ› ï¸ Tool])
    TOOL --> OBS([ğŸ‘ï¸ Observation])
    OBS --> T
    T -- Finish --> A([ğŸ“ Answer])

    %% Colors & Style
    style Q fill:#fca5a5,stroke:#ef4444,stroke-width:2px,color:#000
    style T fill:#86efac,stroke:#22c55e,stroke-width:2px,color:#000
    style TOOL fill:#e5e7eb,stroke:#6b7280,stroke-width:2px,color:#000
    style OBS fill:#e0f2fe,stroke:#38bdf8,stroke-width:2px,color:#000
    style A fill:#fde68a,stroke:#f59e0b,stroke-width:2px,color:#000


```

---

### ğŸ§± Tech Stack Overview

| Layer                    | Tool / Framework                     | Purpose                                                    |
|--------------------------|--------------------------------------|------------------------------------------------------------|
| Document Ingestion       | `SimpleDirectoryReader` (LlamaIndex) | Load raw files from local directory                        |
| Chunking & Embedding     | `SentenceSplitter`, `HuggingFaceEmbedding` | Convert docs to semantic chunks and vectors       |
| Vector Storage           | `ChromaVectorStore` + Chroma DB      | Store and query dense embeddings                           |
| Semantic Search          | `VectorIndexRetriever`               | Retrieve relevant chunks based on vector similarity        |
| Tool Abstraction         | LangChain `Tool`                     | Wrap retriever and search as callable tools                |
| External Knowledge Tool  | `DuckDuckGoSearchRun`                | Perform fallback searches for general queries              |
| Reasoning Engine         | `ReAct Agent` (LangChain)            | Enable step-by-step tool selection and execution           |
| Language Model           | `OpenAI`                             | Used for generation and reasoning                          |

---

### ğŸ§© Step-by-step Breakdown with Code

#### ğŸŸ¡ Step 1: Load and Embed Documents
```python
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.embeddings import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
import chromadb

def load_and_index_docs(folder_path):
    documents = SimpleDirectoryReader(folder_path).load_data()
    node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)
    embed_model = HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
    service_context = ServiceContext.from_defaults(embed_model=embed_model, node_parser=node_parser)
    chroma_client = chromadb.Client()
    vector_store = ChromaVectorStore(chroma_collection=chroma_client.create_collection("rag_docs"))
    index = VectorStoreIndex.from_documents(documents, service_context=service_context, vector_store=vector_store)
    return index
```

#### ğŸŸ¡ Step 2: Create Query Engine & Retriever
```python
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever

def setup_query_engine(index):
    retriever = VectorIndexRetriever(index=index, similarity_top_k=5)
    query_engine = RetrieverQueryEngine(retriever=retriever)
    return query_engine, retriever
```

#### ğŸŸ¡ Step 3: Wrap Retriever into LangChain Tool
```python
from langchain.agents import Tool

def build_qa_tool(query_engine):
    def query_fn(q):
        return query_engine.query(q).response

    return Tool(
        name="knowledge_base_lookup",
        func=query_fn,
        description="Use this tool to query internal knowledge base."
    )
```

#### ğŸŸ¡ Step 4: Add Web Search Tool (DuckDuckGo)
```python
from langchain.tools import DuckDuckGoSearchRun

def build_search_tool():
    search = DuckDuckGoSearchRun()
    return Tool(
        name="web_search",
        func=search.run,
        description="Use this tool to search external general knowledge."
    )
```

#### ğŸŸ¡ Step 5: Setup ReAct Agent (Multi-hop Execution Enabled)
```python
from langchain.agents import initialize_agent, AgentType
from langchain.llms import OpenAI

def build_react_agent(tools):
    return initialize_agent(
        tools=tools,
        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
        llm=OpenAI(temperature=0),
        verbose=True  # Show reasoning steps
    )
```

#### ğŸŸ¡ Step 6: Ask a Complex, Multi-hop Question
```python
def multi_hop_reasoning(agent, question):
    print("\nğŸ¤” Reasoning Trace:")
    return agent.run(question)
```

#### ğŸ§ª Step 7: Main Pipeline Runner
```python
if __name__ == "__main__":
    index = load_and_index_docs("./data")
    query_engine, retriever = setup_query_engine(index)
    qa_tool = build_qa_tool(query_engine)
    search_tool = build_search_tool()
    agent = build_react_agent([qa_tool, search_tool])

    question = (
        "What are the key differences between Pinecone and Weaviate in terms of scalability and "
        "search algorithm, and how should I choose between them for a multi-tenant architecture?"
    )

    final_answer = multi_hop_reasoning(agent, question)
    print("\nğŸ§  Final Answer:\n", final_answer)
```

---

### âœ… Notes on Multi-hop Reasoning
- The ReAct agent interprets the complex query and performs **step-by-step tool calls**.
- Intermediate reasoning ("Thought:", "Action:") is visible thanks to `verbose=True`.
- Example output clearly shows **multi-hop trace**:
```
> Thought: I need to know the vector DBs used for multi-tenant search
> Action: knowledge_base_lookup
...
> Thought: Let's check Weaviateâ€™s open-source scalability limits
> Action: web_search
...
```
---
</details>

---

## ğŸ” Evaluation and Optimization

Äá»ƒ Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ truy xuáº¥t trong há»‡ thá»‘ng RAG, chÃºng tÃ´i sá»­ dá»¥ng táº­p dá»¯ liá»‡u multi-hop tá»« [MultiHop-RAG](https://openreview.net/pdf?id=t4eB3zYWBK), trong Ä‘Ã³ má»—i truy váº¥n yÃªu cáº§u nhiá»u Ä‘oáº¡n context liÃªn quan. VÃ¬ váº­y, framework nhÆ° **RAGAS** khÃ´ng phÃ¹ há»£p do giáº£ Ä‘á»‹nh chá»‰ má»™t Ä‘oáº¡n context Ä‘Ãºng.

### ğŸ¯ Evaluation Metrics

ChÃºng tÃ´i sá»­ dá»¥ng 4 chá»‰ sá»‘ phá»• biáº¿n trong Ä‘Ã¡nh giÃ¡ há»‡ thá»‘ng truy xuáº¥t thÃ´ng tin:

---

### ğŸ“Œ **Recall\@k**

> Äo tá»· lá»‡ tÃ i liá»‡u liÃªn quan trong táº­p top-k Ä‘Æ°á»£c truy xuáº¥t so vá»›i tá»•ng sá»‘ tÃ i liá»‡u ground-truth.

**CÃ´ng thá»©c:**

```math
Recall@k = \frac{|\text{Retrieved@k} \cap \text{GroundTruth}|}{|\text{GroundTruth}|}
```

---

### ğŸ“Œ **Precision\@k**

> Äo tá»· lá»‡ tÃ i liá»‡u liÃªn quan trong top-k káº¿t quáº£ so vá»›i tá»•ng sá»‘ tÃ i liá»‡u Ä‘Æ°á»£c truy xuáº¥t.

```math
Precision@k = \frac{|\text{Retrieved@k} \cap \text{GroundTruth}|}{k}
```

---

### ğŸ“Œ **MAP\@k** (Mean Average Precision)

> Trung bÃ¬nh cÃ¡c Precision tÃ­nh táº¡i tá»«ng vá»‹ trÃ­ cÃ³ tÃ i liá»‡u liÃªn quan.

```math
AP@k = \frac{1}{|\text{GT}|} \sum_{j=1}^{k} P(j) \cdot rel(j)
```

```math
MAP@k = \frac{1}{N} \sum_{i=1}^{N} AP@k_i
```

---

### ğŸ“Œ **Hits\@k**

> Äo tá»· lá»‡ truy váº¥n mÃ  Ã­t nháº¥t má»™t tÃ i liá»‡u liÃªn quan náº±m trong top-k.

```math
Hits@k_i =
\begin{cases}
1 & \text{if } \exists r \in \text{Retrieved@k}_i \cap \text{GT}_i \\
0 & \text{otherwise}
\end{cases}
```

```math
Hits@k = \frac{1}{N} \sum_{i=1}^{N} Hits@k_i
```

---

## ğŸ”§ Setup

```python
from langchain_community.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.retrievers import BM25Retriever
from langchain.schema import Document

from datasets import Dataset
import concurrent.futures
from typing import List, Dict, Tuple

# Embedding model
embedding = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",
    model_kwargs={"device": "cuda"}
)

# Build vector store
vectorstore = Chroma.from_texts(
    texts=texts,
    embedding=embedding,
    persist_directory="./chroma_store_db"
)

# Build BM25 retriever for hybrid
documents = [Document(page_content=text) for text in texts]
bm25_retriever = BM25Retriever.from_documents(documents, k=4)
```

---

## âœ… Baseline: Semantic Search Only
Sá»­ dá»¥ng mÃ´ hÃ¬nh embedding BAAI/bge-large-en-v1.5 vá»›i vector DB Chroma, vÃ  thá»±c hiá»‡n truy váº¥n top-k=4 báº±ng semantic search.
```python
def semantic_retrieve(query: str, k: int = 4) -> List[str]:
    docs = vectorstore.similarity_search(query, k=k)
    return [doc.page_content for doc in docs]

def process_question_semantic(item: Dict) -> tuple:
    question = item["question"]
    contexts = semantic_retrieve(question, k=4)
    return question, contexts, item["contexts"]

def process_dataset_semantic(data: List[Dict]) -> Tuple:
    questions, contexts, gts = [], [], []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(process_question_semantic, item) for item in data]
        for f in concurrent.futures.as_completed(futures):
            q, ctxs, gt = f.result()
            questions.append(q)
            contexts.append(ctxs)
            gts.append(gt)
    return questions, contexts, gts
```

**Káº¿t quáº£:**

```text
Recall@4:     0.5197
Precision@4:  0.3320
MAP@4:        0.4135
Hits@4:       0.8440
```

---

## ğŸ”€ Optimization: Hybrid Search (BM25 + Dense)
Äá»ƒ cáº£i thiá»‡n káº¿t quáº£, chÃºng tÃ´i Ã¡p dá»¥ng ká»¹ thuáº­t **Hybrid Search** báº±ng cÃ¡ch káº¿t há»£p hai phÆ°Æ¡ng phÃ¡p:

* **BM25 (sparse retrieval)** â€” sá»­ dá»¥ng keyword matching
* **Dense (semantic retrieval)** â€” dÃ¹ng embedding semantic similarity

Sá»­ dá»¥ng cÃ´ng thá»©c **score interpolation** vá»›i tham sá»‘ `alpha = 0.2`:

$$
\text{HybridScore} = \alpha \cdot s_{\text{dense}} + (1 - \alpha) \cdot s_{\text{bm25}}
$$

Trong Ä‘Ã³:

- $s_{\text{dense}}$: normalized score tá»« semantic search
* $s_{\text{bm25}}$: normalized score tá»« BM25 

```python
def normalize_scores_with_rank(docs: List[Document], source: str) -> Dict[str, Dict]:
    n = len(docs)
    return {
        doc.page_content: {"doc": doc, "score": 1.0 - (i / (n - 1)), "source": source}
        for i, doc in enumerate(docs)
    } if n > 1 else {
        doc.page_content: {"doc": doc, "score": 1.0, "source": source}
        for doc in docs
    }

def normalize_scores_with_value(docs_with_scores: List[Tuple[Document, float]], source: str) -> Dict[str, Dict]:
    if not docs_with_scores:
        return {}
    scores = [s for _, s in docs_with_scores]
    min_s, max_s = min(scores), max(scores)
    range_s = max_s - min_s if max_s != min_s else 1.0
    return {
        doc.page_content: {"doc": doc, "score": (score - min_s) / range_s, "source": source}
        for doc, score in docs_with_scores
    }

def hybrid_retrieve(query: str, k: int = 4, alpha: float = 0.2) -> List[str]:
    bm25_docs = bm25_retriever.get_relevant_documents(query)
    dense_docs = vectorstore.similarity_search_with_score(query, k=k)

    bm25_dict = normalize_scores_with_rank(bm25_docs, "bm25")
    dense_dict = normalize_scores_with_value(dense_docs, "dense")

    all_keys = set(bm25_dict.keys()) | set(dense_dict.keys())
    scored = []
    for key in all_keys:
        s_bm25 = bm25_dict.get(key, {"score": 0.0})["score"]
        s_dense = dense_dict.get(key, {"score": 0.0})["score"]
        score = alpha * s_dense + (1 - alpha) * s_bm25
        doc = dense_dict.get(key, bm25_dict.get(key))["doc"]
        scored.append((doc, score))

    top_docs = sorted(scored, key=lambda x: x[1], reverse=True)[:k]
    return [doc.page_content for doc, _ in top_docs]
```

### ğŸ” Evaluation

```python
def process_question_hybrid(item: Dict) -> tuple:
    question = item["question"]
    contexts = hybrid_retrieve(question, k=4, alpha=0.2)
    return question, contexts, item["contexts"]

def process_dataset_hybrid(data: List[Dict]) -> Tuple:
    questions, contexts, gts = [], [], []
    with concurrent.futures.ThreadPoolExecutor() as executor:
        futures = [executor.submit(process_question_hybrid, item) for item in data]
        for f in concurrent.futures.as_completed(futures):
            q, ctxs, gt = f.result()
            questions.append(q)
            contexts.append(ctxs)
            gts.append(gt)
    return questions, contexts, gts
```

**Káº¿t quáº£:**

```text
Recall@4:     0.5283
Precision@4:  0.3330
MAP@4:        0.4262
Hits@4:       0.8580
```

---

## ğŸ“Š So sÃ¡nh Káº¿t quáº£

| Metric       | Semantic Only | Hybrid (Î±=0.2) |
| ------------ | ------------- | -------------- |
| Recall\@4    | 0.5197        | **0.5283**     |
| Precision\@4 | 0.3320        | **0.3330**     |
| MAP\@4       | 0.4135        | **0.4262**     |
| Hits\@4      | 0.8440        | **0.8580**     |

---

## ğŸ§  Nháº­n xÃ©t

Viá»‡c Ã¡p dá»¥ng ká»¹ thuáº­t **hybrid retrieval** vá»›i trá»ng sá»‘ Î± = 0.2 Ä‘Ã£ giÃºp cáº£i thiá»‡n hiá»‡u suáº¥t truy xuáº¥t á»Ÿ táº¥t cáº£ cÃ¡c chá»‰ sá»‘:

* **Recall** vÃ  **MAP** cáº£i thiá»‡n rÃµ rá»‡t, cho tháº¥y hybrid retrieval giÃºp há»‡ thá»‘ng bao phá»§ nhiá»u context Ä‘Ãºng hÆ¡n.
* **Hits\@k** tÄƒng nháº¹, chá»©ng minh ráº±ng kháº£ nÄƒng láº¥y Ã­t nháº¥t má»™t context Ä‘Ãºng Ä‘Ã£ Ä‘Æ°á»£c cáº£i thiá»‡n.
* **Precision** tÄƒng nháº¹, khÃ´ng Ä‘Ã¡ng ká»ƒ do k cá»‘ Ä‘á»‹nh lÃ  4.

> Hybrid search lÃ  má»™t ká»¹ thuáº­t Ä‘Æ¡n giáº£n nhÆ°ng hiá»‡u quáº£ Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ BM25 (sparse lexical) vÃ  semantic retrieval (dense vector). ÄÃ¢y lÃ  bÆ°á»›c tá»‘i Æ°u Ä‘áº§u tiÃªn nÃªn thá»±c hiá»‡n trong báº¥t ká»³ há»‡ thá»‘ng RAG thá»±c táº¿ nÃ o.

---
</details>
